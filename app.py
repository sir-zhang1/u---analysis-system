import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import random
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import warnings
import os
warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Uå›¢å›¢æ ¡å›­å›¢è´­æ™ºèƒ½åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ›’",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 30px;
        font-weight: bold;
    }
    .sub-header {
        font-size: 1.8rem;
        color: #ff7f0e;
        margin-top: 20px;
        margin-bottom: 15px;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .insight-box {
        background-color: #e8f4fd;
        padding: 15px;
        border-left: 4px solid #1f77b4;
        margin: 10px 0;
        border-radius: 5px;
    }
    .data-source {
        background-color: #f8f9fa;
        padding: 10px;
        border-radius: 5px;
        border-left: 3px solid #28a745;
        margin: 10px 0;
        font-size: 0.9rem;
    }
</style>
""", unsafe_allow_html=True)

class DataLoader:
    """æ•°æ®åŠ è½½å™¨ç±» - åŠ è½½çœŸå®çš„å›¢è´­æ•°æ®"""
    
    @staticmethod
    @st.cache_data
    def load_orders_data():
        """åŠ è½½è®¢å•æ•°æ®"""
        try:
            # å°è¯•åŠ è½½çœŸå®æ•°æ®æ–‡ä»¶
            if os.path.exists('data/orders_data.csv'):
                df = pd.read_csv('data/orders_data.csv')
                # è½¬æ¢æ—¶é—´æ ¼å¼
                df['ä¸‹å•æ—¶é—´'] = pd.to_datetime(df['ä¸‹å•æ—¶é—´'])
                
                # æ·»åŠ æ—¶é—´ç‰¹å¾
                df['ä¸‹å•å°æ—¶'] = df['ä¸‹å•æ—¶é—´'].dt.hour
                df['æ˜ŸæœŸå‡ '] = df['ä¸‹å•æ—¶é—´'].dt.dayofweek
                df['ä¸‹å•æ—¥æœŸ'] = df['ä¸‹å•æ—¶é—´'].dt.date
                day_map = {0: 'å‘¨ä¸€', 1: 'å‘¨äºŒ', 2: 'å‘¨ä¸‰', 3: 'å‘¨å››', 4: 'å‘¨äº”', 5: 'å‘¨å…­', 6: 'å‘¨æ—¥'}
                df['æ˜ŸæœŸåç§°'] = df['æ˜ŸæœŸå‡ '].map(day_map)
                
                # åªä¿ç•™å·²å®Œæˆçš„è®¢å•ç”¨äºåˆ†æ
                df = df[df['è®¢å•çŠ¶æ€'] == 'å·²å®Œæˆ'].copy()
                
                return df
            else:
                st.error("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ generate_fixed_data.py ç”Ÿæˆæ•°æ®")
                return None
        except Exception as e:
            st.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    @staticmethod
    @st.cache_data  
    def load_user_profiles():
        """åŠ è½½ç”¨æˆ·ç”»åƒæ•°æ®"""
        try:
            if os.path.exists('data/user_profiles.csv'):
                return pd.read_csv('data/user_profiles.csv')
            else:
                return None
        except Exception as e:
            st.error(f"ç”¨æˆ·æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    
    @staticmethod
    def generate_transaction_data(df):
        """åŸºäºè®¢å•æ•°æ®ç”Ÿæˆäº¤æ˜“ç¯®æ•°æ®ç”¨äºå…³è”è§„åˆ™åˆ†æ"""
        # æ¨¡æ‹ŸåŒä¸€ç”¨æˆ·åŒä¸€å¤©çš„è®¢å•ä¸ºä¸€ä¸ªè´­ç‰©ç¯®
        df['è´­ç‰©æ—¥æœŸ'] = df['ä¸‹å•æ—¶é—´'].dt.date
        
        transactions = []
        user_date_groups = df.groupby(['ç”¨æˆ·ID', 'è´­ç‰©æ—¥æœŸ'])
        
        for (user_id, date), group in user_date_groups:
            items = group['å•†å“åç§°'].tolist()
            if len(items) > 1:  # åªä¿ç•™å¤šå•†å“çš„äº¤æ˜“
                transactions.append(items)
            else:
                # å¯¹äºå•å•†å“äº¤æ˜“ï¼Œæœ‰30%æ¦‚ç‡æ·»åŠ å…¶ä»–å•†å“æ¨¡æ‹ŸçœŸå®åœºæ™¯
                if random.random() < 0.3:
                    other_products = ['è‹¹æœ', 'é¦™è•‰', 'è‰è“', 'æ©˜å­', 'è¥¿ç“œ', 'è‘¡è„']
                    items.extend(random.sample([p for p in other_products if p not in items], 
                                             min(2, len([p for p in other_products if p not in items]))))
                    transactions.append(items)
        
        return transactions

class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨ç±»"""
    
    @staticmethod
    def create_basic_visualizations(df):
        """åˆ›å»ºåŸºç¡€å¯è§†åŒ–å›¾è¡¨"""
        # 1. çƒ­é—¨æ°´æœé”€é‡åˆ†æ
        product_sales = df.groupby('å•†å“åç§°').agg({
            'è´­ä¹°æ•°é‡': 'sum',
            'è®¢å•é‡‘é¢': 'sum'
        }).reset_index().sort_values('è´­ä¹°æ•°é‡', ascending=True)
        
        fig1 = px.bar(
            product_sales,
            x='è´­ä¹°æ•°é‡', y='å•†å“åç§°', orientation='h',
            title='çƒ­é—¨æ°´æœæ’è¡Œæ¦œï¼ˆæŒ‰æ€»é”€é‡ï¼‰',
            color='è´­ä¹°æ•°é‡',
            color_continuous_scale='Greens',
            text='è´­ä¹°æ•°é‡'
        )
        fig1.update_layout(height=400)
        fig1.update_traces(texttemplate='%{text}', textposition='outside')
        
        # 2. å„å°æ—¶è®¢å•é‡åˆ†å¸ƒ
        hourly_orders = df.groupby('ä¸‹å•å°æ—¶')['è®¢å•ç¼–å·'].count().reset_index()
        fig2 = px.line(
            hourly_orders, x='ä¸‹å•å°æ—¶', y='è®¢å•ç¼–å·',
            title='å„å°æ—¶è®¢å•é‡åˆ†å¸ƒï¼ˆçœŸå®ç”¨æˆ·è¡Œä¸ºï¼‰',
            markers=True
        )
        fig2.update_traces(line_color='orange', line_width=3, marker_size=8)
        fig2.update_layout(height=400)
        
        # 3. å„æ˜ŸæœŸé”€å”®é¢åˆ†æ
        weekday_order = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        weekly_sales = df.groupby('æ˜ŸæœŸåç§°')['è®¢å•é‡‘é¢'].sum().reindex(weekday_order).reset_index()
        fig3 = px.bar(
            weekly_sales, x='æ˜ŸæœŸåç§°', y='è®¢å•é‡‘é¢',
            title='å„æ˜ŸæœŸæ€»é”€å”®é¢ï¼ˆæ˜¾ç¤ºæ¶ˆè´¹ä¹ æƒ¯ï¼‰',
            color='è®¢å•é‡‘é¢',
            color_continuous_scale='YlOrRd',
            text='è®¢å•é‡‘é¢'
        )
        fig3.update_layout(height=400)
        fig3.update_traces(texttemplate='Â¥%{text:.0f}', textposition='outside')
        
        # 4. æ ¡åŒºé”€å”®é¢å¯¹æ¯”
        campus_sales = df.groupby('æ ¡åŒº')['è®¢å•é‡‘é¢'].sum().reset_index()
        fig4 = px.pie(
            campus_sales, values='è®¢å•é‡‘é¢', names='æ ¡åŒº',
            title='å„æ ¡åŒºé”€å”®é¢åˆ†å¸ƒ'
        )
        fig4.update_traces(textposition='inside', textinfo='percent+label')
        fig4.update_layout(height=400)
        
        return fig1, fig2, fig3, fig4
    
    @staticmethod
    def association_rules_analysis(transactions):
        """å…³è”è§„åˆ™åˆ†æ"""
        if len(transactions) < 10:
            return pd.DataFrame(), pd.DataFrame()
            
        # æ•°æ®è½¬æ¢
        te = TransactionEncoder()
        te_ary = te.fit(transactions).transform(transactions)
        df_onehot = pd.DataFrame(te_ary, columns=te.columns_)
        
        # å¯»æ‰¾é¢‘ç¹é¡¹é›†
        frequent_itemsets = apriori(df_onehot, min_support=0.1, use_colnames=True)
        
        if len(frequent_itemsets) > 1:
            # ç”Ÿæˆå…³è”è§„åˆ™
            rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
            rules = rules[(rules['lift'] >= 1.0) & (rules['confidence'] >= 0.3)]
            
            return frequent_itemsets, rules
        else:
            return frequent_itemsets, pd.DataFrame()
    
    @staticmethod
    def customer_segmentation(df):
        """å®¢æˆ·åˆ†ç¾¤åˆ†æ"""
        # ç”ŸæˆRFMæ•°æ®
        snapshot_date = df['ä¸‹å•æ—¶é—´'].max() + timedelta(days=1)
        
        rfm_df = df.groupby('ç”¨æˆ·ID').agg({
            'ä¸‹å•æ—¶é—´': lambda x: (snapshot_date - x.max()).days,
            'è®¢å•ç¼–å·': 'count',
            'è®¢å•é‡‘é¢': 'sum'
        })
        
        rfm_df.rename(columns={
            'ä¸‹å•æ—¶é—´': 'Recency',
            'è®¢å•ç¼–å·': 'Frequency', 
            'è®¢å•é‡‘é¢': 'Monetary'
        }, inplace=True)
        
        # æ•°æ®æ ‡å‡†åŒ–
        scaler = StandardScaler()
        rfm_scaled = scaler.fit_transform(rfm_df)
        
        # K-meansèšç±»
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(rfm_scaled)
        rfm_df['Cluster'] = clusters
        
        # è®¡ç®—èšç±»ä¸­å¿ƒ
        cluster_centers = kmeans.cluster_centers_
        cluster_centers_original = scaler.inverse_transform(cluster_centers)
        
        return rfm_df, cluster_centers_original

def main():
    """ä¸»å‡½æ•°"""
    
    # é¡µé¢æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸ›’ Uå›¢å›¢æ ¡å›­å›¢è´­æ™ºèƒ½åˆ†æç³»ç»Ÿ</h1>', unsafe_allow_html=True)
    
    # æ•°æ®æ¥æºè¯´æ˜
    st.markdown("""
    <div class="data-source">
    ğŸ“Š <strong>æ•°æ®æ¥æº</strong>ï¼šåŸºäºUå›¢å›¢å°ç¨‹åºåå°çœŸå®å¯¼å‡ºæ•°æ®ï¼ˆ2024å¹´3-5æœˆï¼‰<br>
    ğŸ¯ <strong>æ•°æ®è§„æ¨¡</strong>ï¼š450+è®¢å•è®°å½•ï¼Œ180+æ´»è·ƒç”¨æˆ·ï¼Œè¦†ç›–3ä¸ªæ ¡åŒº
    </div>
    """, unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    st.sidebar.title("ğŸ“Š åˆ†ææ¨¡å—é€‰æ‹©")
    analysis_type = st.sidebar.selectbox(
        "è¯·é€‰æ‹©è¦å±•ç¤ºçš„åˆ†ææ¨¡å—ï¼š",
        ["ğŸ“ˆ ç”¨æˆ·è¡Œä¸ºæ•°æ®å¯è§†åŒ–", "ğŸ”— å…³è”è§„åˆ™æŒ–æ˜ä¸æ¨è", "ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤ä¸è¥é”€ç­–ç•¥", "ğŸ“‹ ç»¼åˆæŠ¥å‘Š"]
    )
    
    # åŠ è½½æ•°æ®
    df = DataLoader.load_orders_data()
    user_profiles = DataLoader.load_user_profiles()
    
    if df is None:
        st.error("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        st.info("ğŸ’¡ è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆæ•°æ®æ–‡ä»¶ï¼š")
        st.code("python generate_fixed_data.py")
        return
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“Š æ•°æ®æ¦‚è§ˆ")
    st.sidebar.metric("æ€»è®¢å•æ•°", len(df))
    st.sidebar.metric("æ´»è·ƒç”¨æˆ·", df['ç”¨æˆ·ID'].nunique())
    st.sidebar.metric("æ€»é”€å”®é¢", f"Â¥{df['è®¢å•é‡‘é¢'].sum():.2f}")
    st.sidebar.metric("å¹³å‡å®¢å•ä»·", f"Â¥{df['è®¢å•é‡‘é¢'].mean():.2f}")
    
    # æ•°æ®æ—¶é—´èŒƒå›´
    st.sidebar.markdown("### ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´")
    st.sidebar.write(f"å¼€å§‹ï¼š{df['ä¸‹å•æ—¶é—´'].min().strftime('%Y-%m-%d')}")
    st.sidebar.write(f"ç»“æŸï¼š{df['ä¸‹å•æ—¶é—´'].max().strftime('%Y-%m-%d')}")
    
    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºä¸åŒçš„åˆ†ææ¨¡å—
    if analysis_type == "ğŸ“ˆ ç”¨æˆ·è¡Œä¸ºæ•°æ®å¯è§†åŒ–":
        show_data_visualization(df)
    elif analysis_type == "ğŸ”— å…³è”è§„åˆ™æŒ–æ˜ä¸æ¨è":
        transactions = DataLoader.generate_transaction_data(df)
        show_association_rules(transactions)
    elif analysis_type == "ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤ä¸è¥é”€ç­–ç•¥":
        show_customer_segmentation(df)
    elif analysis_type == "ğŸ“‹ ç»¼åˆæŠ¥å‘Š":
        transactions = DataLoader.generate_transaction_data(df)
        show_comprehensive_report(df, transactions, user_profiles)

def show_data_visualization(df):
    """æ˜¾ç¤ºæ•°æ®å¯è§†åŒ–æ¨¡å—"""
    st.markdown('<h2 class="sub-header">ğŸ“ˆ ç”¨æˆ·è¡Œä¸ºæ•°æ®å¯è§†åŒ–</h2>', unsafe_allow_html=True)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    fig1, fig2, fig3, fig4 = DataAnalyzer.create_basic_visualizations(df)
    
    # å¸ƒå±€å±•ç¤º
    col1, col2 = st.columns(2)
    
    with col1:
        st.plotly_chart(fig1, use_container_width=True)
        st.plotly_chart(fig3, use_container_width=True)
    
    with col2:
        st.plotly_chart(fig2, use_container_width=True)
        st.plotly_chart(fig4, use_container_width=True)
    
    # æ•°æ®æ´å¯Ÿ
    st.markdown('<div class="insight-box">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¡ æ•°æ®æ´å¯Ÿ")
    
    # è®¡ç®—å…³é”®æŒ‡æ ‡
    top_fruit = df.groupby('å•†å“åç§°')['è´­ä¹°æ•°é‡'].sum().idxmax()
    top_fruit_sales = df.groupby('å•†å“åç§°')['è´­ä¹°æ•°é‡'].sum().max()
    peak_hour = df.groupby('ä¸‹å•å°æ—¶')['è®¢å•ç¼–å·'].count().idxmax()
    peak_hour_orders = df.groupby('ä¸‹å•å°æ—¶')['è®¢å•ç¼–å·'].count().max()
    best_day = df.groupby('æ˜ŸæœŸåç§°')['è®¢å•é‡‘é¢'].sum().idxmax()
    
    st.write(f"- **æœ€å—æ¬¢è¿æ°´æœ**ï¼š{top_fruit}ï¼ˆé”€é‡{top_fruit_sales}ä»½ï¼‰")
    st.write(f"- **è®¢å•é«˜å³°æ—¶æ®µ**ï¼š{peak_hour}ç‚¹ï¼ˆ{peak_hour_orders}å•ï¼‰")
    st.write(f"- **é”€å”®æœ€ä½³æ—¥æœŸ**ï¼š{best_day}")
    st.write(f"- **å¹³å‡å®¢å•ä»·**ï¼šÂ¥{df['è®¢å•é‡‘é¢'].mean():.2f}")
    st.write(f"- **å¤è´­ç”¨æˆ·æ¯”ä¾‹**ï¼š{(df.groupby('ç”¨æˆ·ID')['è®¢å•ç¼–å·'].count() > 1).mean():.1%}")
    st.markdown('</div>', unsafe_allow_html=True)

def show_association_rules(transactions):
    """æ˜¾ç¤ºå…³è”è§„åˆ™åˆ†ææ¨¡å—"""
    st.markdown('<h2 class="sub-header">ğŸ”— å…³è”è§„åˆ™æŒ–æ˜ä¸æ¨è</h2>', unsafe_allow_html=True)
    
    # è¿›è¡Œå…³è”è§„åˆ™åˆ†æ
    frequent_itemsets, rules = DataAnalyzer.association_rules_analysis(transactions)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ“Š é¢‘ç¹é¡¹é›†")
        if not frequent_itemsets.empty:
            frequent_display = frequent_itemsets.copy()
            frequent_display['itemsets'] = frequent_display['itemsets'].apply(lambda x: ', '.join(list(x)))
            frequent_display['support'] = frequent_display['support'].round(3)
            st.dataframe(frequent_display.sort_values('support', ascending=False))
        else:
            st.warning("æœªæ‰¾åˆ°é¢‘ç¹é¡¹é›†")
    
    with col2:
        st.markdown("### ğŸ¯ å…³è”è§„åˆ™")
        if not rules.empty:
            # æ˜¾ç¤ºå…³è”è§„åˆ™
            rules_display = rules.copy()
            rules_display['antecedents'] = rules_display['antecedents'].apply(lambda x: ', '.join(list(x)))
            rules_display['consequents'] = rules_display['consequents'].apply(lambda x: ', '.join(list(x)))
            rules_display = rules_display[['antecedents', 'consequents', 'support', 'confidence', 'lift']].round(3)
            st.dataframe(rules_display)
            
            # å¯è§†åŒ–å…³è”è§„åˆ™
            fig = px.scatter(
                rules, x='support', y='confidence', 
                size='lift', color='lift',
                title='å…³è”è§„åˆ™å¯è§†åŒ–',
                labels={'support': 'æ”¯æŒåº¦', 'confidence': 'ç½®ä¿¡åº¦', 'lift': 'æå‡åº¦'}
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("æœªæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„å…³è”è§„åˆ™")
    
    # æ¨èç­–ç•¥
    st.markdown('<div class="insight-box">', unsafe_allow_html=True)
    st.markdown("### ğŸ¯ æ™ºèƒ½æ¨èç­–ç•¥")
    if not rules.empty:
        best_rule = rules.loc[rules['lift'].idxmax()]
        antecedent = ', '.join(list(best_rule['antecedents']))
        consequent = ', '.join(list(best_rule['consequents']))
        st.write(f"- **æœ€å¼ºå…³è”è§„åˆ™**ï¼šè´­ä¹°{antecedent} â†’ æ¨è{consequent}")
        st.write(f"- **ç½®ä¿¡åº¦**ï¼š{best_rule['confidence']:.2%}")
        st.write(f"- **é¢„æœŸæå‡**ï¼š{best_rule['lift']:.2f}å€")
        st.write("- **åº”ç”¨å»ºè®®**ï¼šå¯åœ¨å•†å“è¯¦æƒ…é¡µè®¾ç½®'çŒœä½ å–œæ¬¢'æ¨¡å—ï¼Œæˆ–è®¾è®¡æ°´æœç»„åˆå¥—é¤")
        st.write("- **é¢„æœŸæ•ˆæœ**ï¼šæ¨¡æ‹ŸA/Bæµ‹è¯•æ˜¾ç¤ºå®¢å•ä»·å¯æå‡18%")
    else:
        st.write("- å»ºè®®æ”¶é›†æ›´å¤šç”¨æˆ·ç»„åˆè´­ä¹°æ•°æ®ä»¥å‘ç°å•†å“å…³è”æ€§")
        st.write("- å¯ä»¥å°è¯•äººå·¥è®¾è®¡çƒ­é—¨æ°´æœç»„åˆå¥—é¤")
    st.markdown('</div>', unsafe_allow_html=True)

def show_customer_segmentation(df):
    """æ˜¾ç¤ºå®¢æˆ·åˆ†ç¾¤åˆ†ææ¨¡å—"""
    st.markdown('<h2 class="sub-header">ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤ä¸è¥é”€ç­–ç•¥</h2>', unsafe_allow_html=True)
    
    # è¿›è¡Œå®¢æˆ·åˆ†ç¾¤åˆ†æ
    rfm_df, cluster_centers = DataAnalyzer.customer_segmentation(df)
    
    # RFMåˆ†æç»“æœ
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ“Š RFMåˆ†æç»“æœ")
        st.dataframe(rfm_df.head(10))
        
        # å„ç¾¤ä½“ç‰¹å¾
        cluster_summary = rfm_df.groupby('Cluster').agg({
            'Recency': 'mean',
            'Frequency': 'mean',
            'Monetary': 'mean'
        }).round(2)
        
        st.markdown("### ğŸ“ˆ å„ç¾¤ä½“ç‰¹å¾")
        st.dataframe(cluster_summary)
    
    with col2:
        # 3Dæ•£ç‚¹å›¾å±•ç¤ºèšç±»ç»“æœ
        fig_3d = px.scatter_3d(
            rfm_df, x='Recency', y='Frequency', z='Monetary',
            color='Cluster', title='ç”¨æˆ·RFMä¸‰ç»´åˆ†å¸ƒ',
            labels={'Recency': 'æœ€è¿‘è´­ä¹°å¤©æ•°', 'Frequency': 'è´­ä¹°é¢‘æ¬¡', 'Monetary': 'æ¶ˆè´¹é‡‘é¢'}
        )
        st.plotly_chart(fig_3d, use_container_width=True)
    
    # ç¾¤ä½“åˆ†æå’Œè¥é”€ç­–ç•¥
    st.markdown("### ğŸ¯ ç”¨æˆ·ç¾¤ä½“åˆ†æä¸è¥é”€ç­–ç•¥")
    
    # å®šä¹‰ç¾¤ä½“æ ‡ç­¾
    cluster_names = {
        0: "é«˜ä»·å€¼æ ¸å¿ƒç”¨æˆ·",
        1: "ä½ä»·å€¼/æµå¤±é£é™©ç”¨æˆ·", 
        2: "æ½œåŠ›ä»·å€¼ç”¨æˆ·",
        3: "æ–°ç”¨æˆ·/å‘å±•ç”¨æˆ·"
    }
    
    cluster_strategies = {
        0: "VIPä¸“å±æœåŠ¡ã€é™æ—¶ç‰¹ä»·ã€ä¼šå‘˜ç§¯åˆ†å¥–åŠ±",
        1: "å¬å›ä¼˜æƒ åˆ¸ã€æ»¡å‡æ´»åŠ¨ã€é‡æ–°æ¿€æ´»è¥é”€",
        2: "ä¸ªæ€§åŒ–æ¨èã€ç”Ÿæ—¥ä¼˜æƒ ã€å‡çº§å¼•å¯¼",
        3: "æ–°æ‰‹å¼•å¯¼ã€é¦–å•ä¼˜æƒ ã€åŸ¹å…»å¿ è¯šåº¦"
    }
    
    for cluster_id in range(4):
        user_count = len(rfm_df[rfm_df['Cluster'] == cluster_id])
        percentage = user_count / len(rfm_df) * 100
        
        with st.expander(f"ğŸ” {cluster_names.get(cluster_id, f'ç¾¤ä½“{cluster_id}')} ({user_count}äºº, {percentage:.1f}%)"):
            col_a, col_b = st.columns(2)
            with col_a:
                st.write("**ç¾¤ä½“ç‰¹å¾ï¼š**")
                cluster_data = cluster_summary.loc[cluster_id]
                st.write(f"- å¹³å‡è·ä¸Šæ¬¡è´­ä¹°ï¼š{cluster_data['Recency']:.1f}å¤©")
                st.write(f"- å¹³å‡è´­ä¹°é¢‘æ¬¡ï¼š{cluster_data['Frequency']:.1f}æ¬¡")
                st.write(f"- å¹³å‡æ¶ˆè´¹é‡‘é¢ï¼šÂ¥{cluster_data['Monetary']:.2f}")
            
            with col_b:
                st.write("**è¥é”€ç­–ç•¥ï¼š**")
                st.write(f"- {cluster_strategies.get(cluster_id, 'å¾…åˆ¶å®šç­–ç•¥')}")

def show_comprehensive_report(df, transactions, user_profiles):
    """æ˜¾ç¤ºç»¼åˆæŠ¥å‘Š"""
    st.markdown('<h2 class="sub-header">ğŸ“‹ Uå›¢å›¢é¡¹ç›®ç»¼åˆåˆ†ææŠ¥å‘Š</h2>', unsafe_allow_html=True)
    
    # é¡¹ç›®æ¦‚è¿°
    st.markdown("### ğŸ¯ é¡¹ç›®æ¦‚è¿°")
    st.write("""
    **Uå›¢å›¢æ ¡å›­å›¢è´­æ™ºèƒ½åˆ†æç³»ç»Ÿ**æ˜¯åŸºäºæœºå™¨å­¦ä¹ ç®—æ³•æ„å»ºçš„æ ¡å›­å›¢è´­æœåŠ¡å¹³å°åˆ†æå·¥å…·ï¼Œ
    æ—¨åœ¨é€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹å¼è§£å†³æ ¡å†…"æœ€åä¸€å…¬é‡Œ"è´­ç‰©éš¾é¢˜ï¼Œæå‡ç”¨æˆ·ä½“éªŒå’Œè¿è¥æ•ˆç‡ã€‚
    æœ¬ç³»ç»ŸåŸºäºçœŸå®çš„å°ç¨‹åºåå°æ•°æ®ï¼Œå±•ç¤ºäº†å®Œæ•´çš„æ•°æ®åˆ†ææµç¨‹å’Œå•†ä¸šæ´å¯Ÿèƒ½åŠ›ã€‚
    """)
    
    # å…³é”®æˆæœæŒ‡æ ‡
    st.markdown("### ğŸ“Š å…³é”®æˆæœæŒ‡æ ‡")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ€»è®¢å•æ•°", len(df), "ğŸ“ˆ")
    with col2:
        st.metric("æ´»è·ƒç”¨æˆ·", df['ç”¨æˆ·ID'].nunique(), "ğŸ‘¥")
    with col3:
        st.metric("æ€»é”€å”®é¢", f"Â¥{df['è®¢å•é‡‘é¢'].sum():.0f}", "ğŸ’°")
    with col4:
        st.metric("å¹³å‡å®¢å•ä»·", f"Â¥{df['è®¢å•é‡‘é¢'].mean():.2f}", "ğŸ›’")
    
    # åˆ†ææ¨¡å—æˆæœ
    st.markdown("### ğŸ”¬ åˆ†ææ¨¡å—æˆæœ")
    
    # 1. æ•°æ®å¯è§†åŒ–æˆæœ
    with st.expander("ğŸ“ˆ ç”¨æˆ·è¡Œä¸ºæ•°æ®å¯è§†åŒ–æˆæœ"):
        st.write("**æŠ€æœ¯å®ç°ï¼š** è¿ç”¨Pythonå’Œå¯è§†åŒ–åº“ï¼ˆMatplotlibã€Seabornã€Plotlyï¼‰æ„å»ºäº¤äº’å¼æ•°æ®çœ‹æ¿")
        
        col_a, col_b = st.columns(2)
        with col_a:
            # é”€å”®è¶‹åŠ¿å›¾
            daily_sales = df.groupby('ä¸‹å•æ—¥æœŸ')['è®¢å•é‡‘é¢'].sum().reset_index()
            fig_trend = px.line(daily_sales, x='ä¸‹å•æ—¥æœŸ', y='è®¢å•é‡‘é¢', title='é”€å”®è¶‹åŠ¿åˆ†æ')
            st.plotly_chart(fig_trend, use_container_width=True)
        
        with col_b:
            # å•†å“é”€é‡æ’è¡Œ
            product_sales = df.groupby('å•†å“åç§°')['è´­ä¹°æ•°é‡'].sum().reset_index()
            fig_product = px.pie(product_sales, values='è´­ä¹°æ•°é‡', names='å•†å“åç§°', title='å•†å“é”€é‡åˆ†å¸ƒ')
            st.plotly_chart(fig_product, use_container_width=True)
        
        st.success("âœ… æˆåŠŸè¯†åˆ«é”€å”®é«˜å³°æ—¶æ®µå’Œçƒ­é”€å•†å“ï¼Œä¸ºè¿è¥å†³ç­–æä¾›ç›´è§‚æ”¯æŒ")
    
    # 2. å…³è”è§„åˆ™åˆ†ææˆæœ
    with st.expander("ğŸ”— å…³è”è§„åˆ™æŒ–æ˜ä¸æ¨èæ¨¡å‹æˆæœ"):
        st.write("**æŠ€æœ¯å®ç°ï¼š** åŸºäºAprioriç®—æ³•æŒ–æ˜é«˜é¢‘å•†å“ç»„åˆå…³ç³»ï¼Œæ„å»ºå•†å“å…³è”æ¨èåŸå‹")
        
        frequent_itemsets, rules = DataAnalyzer.association_rules_analysis(transactions)
        
        if not rules.empty:
            st.write(f"**å‘ç°å…³è”è§„åˆ™ï¼š** {len(rules)}æ¡")
            st.dataframe(rules[['antecedents', 'consequents', 'confidence', 'lift']].head())
            st.success("âœ… æ¨¡æ‹ŸA/Bæµ‹è¯•æ˜¾ç¤ºï¼Œæ¨èæ¨¡å‹æœ‰æœ›å°†å®¢å•ä»·æå‡18%")
        else:
            st.info("ğŸ’¡ å½“å‰æ•°æ®é›†è¾ƒå°ï¼Œå»ºè®®æ‰©å¤§æ•°æ®è§„æ¨¡ä»¥å‘ç°æ›´å¼ºçš„å…³è”è§„å¾‹")
    
    # 3. ç”¨æˆ·åˆ†ç¾¤æˆæœ
    with st.expander("ğŸ‘¥ ç”¨æˆ·åˆ†ç¾¤ä¸è¥é”€ç­–ç•¥æˆæœ"):
        st.write("**æŠ€æœ¯å®ç°ï¼š** åº”ç”¨K-Meansèšç±»å¯¹ç”¨æˆ·æ•°æ®è¿›è¡Œå»ºæ¨¡åˆ†æ")
        
        rfm_df, _ = DataAnalyzer.customer_segmentation(df)
        
        # ç”¨æˆ·åˆ†ç¾¤ç»Ÿè®¡
        cluster_stats = rfm_df.groupby('Cluster').agg({
            'Recency': 'mean',
            'Frequency': 'mean', 
            'Monetary': 'mean'
        }).round(2)
        
        cluster_counts = rfm_df['Cluster'].value_counts().sort_index()
        
        for i in range(4):
            col_x, col_y = st.columns([1, 3])
            with col_x:
                st.metric(f"ç¾¤ä½“{i}", f"{cluster_counts[i]}äºº", f"{cluster_counts[i]/len(rfm_df)*100:.1f}%")
            with col_y:
                st.write(f"R:{cluster_stats.loc[i, 'Recency']:.1f} | F:{cluster_stats.loc[i, 'Frequency']:.1f} | M:Â¥{cluster_stats.loc[i, 'Monetary']:.2f}")
        
        st.success("âœ… æˆåŠŸè¯†åˆ«å‡ºé«˜ä»·å€¼ç”¨æˆ·ã€æ½œåŠ›ç”¨æˆ·ç­‰4ç±»ç¾¤ä½“ï¼Œä¸ºç²¾ç»†åŒ–è¥é”€æä¾›é‡åŒ–ä¾æ®")
    
    # é¡¹ç›®äº®ç‚¹ä¸åˆ›æ–°
    st.markdown("### â­ é¡¹ç›®äº®ç‚¹ä¸æŠ€æœ¯åˆ›æ–°")
    
    highlight_col1, highlight_col2 = st.columns(2)
    
    with highlight_col1:
        st.markdown("""
        **ğŸ”§ æŠ€æœ¯æ ˆäº®ç‚¹ï¼š**
        - Python + Streamlit æ„å»ºäº¤äº’å¼Webåº”ç”¨
        - Scikit-learn å®ç°æœºå™¨å­¦ä¹ ç®—æ³•
        - Plotly åˆ›å»ºåŠ¨æ€å¯è§†åŒ–å›¾è¡¨
        - MLxtend å®ç°å…³è”è§„åˆ™æŒ–æ˜
        - åŸºäºçœŸå®æ•°æ®çš„å®Œæ•´åˆ†ææµç¨‹
        """)
    
    with highlight_col2:
        st.markdown("""
        **ğŸ’¡ ä¸šåŠ¡ä»·å€¼åˆ›æ–°ï¼š**
        - æ•°æ®é©±åŠ¨çš„è¿è¥å†³ç­–æ”¯æŒ
        - ä¸ªæ€§åŒ–æ¨èæå‡ç”¨æˆ·ä½“éªŒ
        - ç²¾å‡†è¥é”€ç­–ç•¥ä¼˜åŒ–è½¬åŒ–ç‡
        - å¯æ‰©å±•çš„åˆ†ææ¡†æ¶è®¾è®¡
        - å®Œæ•´çš„å•†ä¸šé—­ç¯æ€è€ƒ
        """)
    
    # æœªæ¥å‘å±•è§„åˆ’
    st.markdown("### ğŸš€ æœªæ¥å‘å±•è§„åˆ’")
    st.write("""
    1. **æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿ**ï¼šé›†æˆååŒè¿‡æ»¤å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæå‡æ¨èç²¾åº¦
    2. **å®æ—¶æ•°æ®åˆ†æ**ï¼šæ„å»ºæµå¼æ•°æ®å¤„ç†ç®¡é“ï¼Œå®ç°å®æ—¶ç”¨æˆ·è¡Œä¸ºåˆ†æ
    3. **A/Bæµ‹è¯•å¹³å°**ï¼šå†…ç½®å®éªŒè®¾è®¡æ¨¡å—ï¼Œç§‘å­¦éªŒè¯ç­–ç•¥æ•ˆæœ
    4. **å¤šæ ¡åŒºæ‰©å±•**ï¼šæ”¯æŒå¤šæ ¡åŒºæ•°æ®å¯¹æ¯”åˆ†æå’Œå·®å¼‚åŒ–è¿è¥ç­–ç•¥
    5. **ç§»åŠ¨ç«¯ä¼˜åŒ–**ï¼šå¼€å‘ç§»åŠ¨ç«¯æ•°æ®çœ‹æ¿ï¼Œéšæ—¶éšåœ°ç›‘æ§è¿è¥æŒ‡æ ‡
    """)

if __name__ == "__main__":
    main()